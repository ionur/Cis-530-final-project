{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 LucidaGrande;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \\documentclass[12pt]\{article\}\
\\usepackage[\
singlelinecheck=false % <-- important\
]\{caption\}\
\\usepackage[utf8]\{inputenc\}\
\
\\title\{Anonymize Sensitive Information in Medical Health Records\}\
\\author\{ Bhavana Saluja , Gaurav , Srinivasa Sur, Ipek Onur  \}\
\
\\date\{April 2019\}\
\
\\usepackage[justification=centering, skip=0pt]\{caption\}\
\\usepackage\{float\}\
\\restylefloat\{table\}\
\\restylefloat\{figure\}\
\\usepackage\{natbib\}\
\\usepackage\{graphicx\}\
\\usepackage\{xcolor\}\
\\usepackage\{lipsum\}\
\\usepackage\{setspace\}\
\\usepackage[margin=1in]\{geometry\}\
\\usepackage[explicit]\{titlesec\}\
\\titlespacing\\subsubsection\{2pt\}\{8pt plus 4pt minus 2pt\}\{0pt plus 2pt minus 2pt\}\
\\begin\{document\}\
\
\
\\maketitle\
\\section\{Abstract\}\
\
\\section\{Introduction\}\
\\subsection\{What is PHI?\}\
As mentioned on the task page \\cite\{MainAuthor\} , Clinical records with protected health information (PHI) cannot be directly\
shared \'93as is\'94, due to privacy constraints, making it particularly cumbersome to carry out NLP research in\
the medical domain. A necessary precondition for accessing clinical records outside of hospitals is their\
de-identification, i.e., the exhaustive removal, or replacement, of all mentioned PHI phrases.\
\\par\
What is PHI?\
PHI stands for Protected Health Information and is any information in a medical record that can be used\
to identify an individual, and that was created, used, or disclosed in the course of providing a health care\
service, such as a diagnosis or treatment. In other words, PHI is personally identifiable information in\
medical records, including conversations between doctors and nurses about treatment. PHI also includes\
billing information and any patient-identifiable information in a health insurance company's computer\
system.\
Some information that can be considered PHI are Names, Surnames ,Addresses, Hospitals, Professions, Different types of locations (provinces, cities, towns,...), Billing information, Email, Phone records\
\
\\par\
\\newline\
\\subsection\{Formal definition:\}\
\\newline\
1. Entity recognition on the data:\
The goal is to match exactly the beginning and end locations of each PHI entity tag, as well as\
detecting correctly the annotation type. After looking at training data, we see a good amount of\
examples for this task.\
\\newline\
2. Detect sensitive spans:\
The goal is to identify and be able to obfuscate or mask sensitive data, regardless the actual type\
of entity or the correct offset identification of multi-token sensitive phrase mentions. This is a\
comparatively difficult task because of lack of examples in training data.\
\\subsection\{An example of the task:\}\
In this section, we have defined a figure[\\ref\{fig:masking\}] that shows an example of such a task. In the figure[\\ref\{fig:masking\}], we can see that the sensitive data like Name, Hospital Name and SSN Id are intially visible and are masked after applying any approach that masks sensitive data. Formally, A NLP approach identified all the sensitive tags using NER approach or deep nets or n-gram models and so on ... It identified such words and masked them before releasing it to researchers.\
\
\\begin\{figure\}\
\\includegraphics[width=0.9\\textwidth]\{IMG_5977.JPG\}\
\\caption\{A prototype of the Job Information dialog\}\
\\label\{fig:masking\}\
\\end\{figure\}\
\
\
\\subsection\{Why we choose this topic:\}\
\\section\{Literature Review\}\
We have presented below the brief descriptions of the paper we have read. There are 5 papers in total and we have used the paper\\cite\{paper5\} for our baseline implementation.\
\\subsection\{A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation\\cite\{paper1\}\}\
In this paper, the authors have presented a deep learning architecture that uses bi-directional long short-term memory networks (Bi-LSTMs) with variational dropouts and deep contextualized word embeddings while also using components such as traditional word embeddings (Glove),\
character LSTM embeddings and conditional random fields. Their architecture can be broken down into four distinct layers: pre-processing, embeddings, Bi-LSTM and CRF classifier. In the pre-processing layer, they break down the input document into sentences, tokens, and characters. They then use NLTK to generate a part-of-speech (POS) tag for each token. In embedding layer, for each token they concatenate the GloVe (300 dimensions) and ELMo (1024 dimensions) representations to produce a single 1324-dimensional word input vector. The concatenated word vector is then further concatenated with the character embedding vector (50 dimensions), POS one-hot encoded vector (20 dimensions), and the casing embedded vector to produce single 1394-dimensional input vector that is fed to Bi-LSTM layer. Their Bi-LSTM layer consists of 100 hidden units, uses variational dropout and have a dropout probability of 0.5. Their approach in CRF layer is identical to the Bi-LSTM-CRF model by Huang et al (Z. Huang, W. Xu, and K. Yu, \'93Bidirectional lstm-crf models for sequence tagging,\'94 CoRR, 2015).\
\\par\
The two main data sets that they used to evaluate their architecture are the 2014 i2b2 de-identification challenge data set and the nursing notes corpus. On the task of binary F1 scores, their architecture performs similarly to the best scores achieved by other architectures with them having a slight edge on precision metrics. They have achieved precision of 0.9830, recall of 0.9737, and binary F1 score of 0.9783. For the nursing corpora, they managed to best the scores achieved by the deidentify system while also achieving a binary F1 score of over 0.812. For majority of HIPAA-PHI categories, their system performed better than system by Yang et al.\
\
\\subsection\{Anonymization of General Practioner Medical Records\\cite\{paper2\}\}\
The paper aims to develop techniques and methods for semi-automated anonymization of medical record information. Some anonymization challenges, including linguistic issues (e.g. spelling and ambiguity) and determining which parts of the data that is sensitive are also discussed. The paper proposes methods like utilization of database structure, dictionaries, heuristics and natural language processing for anonymizing patient records in general. The dataset is in Norwegian language which means the task is even more challenging since it is different than English in terms of linguistic aspects. Major challenges which are posited are the differences in identity markers  (e.g. Dr. and Mrs.) and hyphenation patterns in Norwegian, unstructured text, no strictly enforced guidelines for how the data should be encoded. \
\\par\
The goal of this paper is to anonymize general practioner data - both structural and free-text. A 6-step anonymization approach is being proposed- \
\\par\
A. Dictionaries - Words and numbers found in structural data can  be extracted into local dictionaries with corresponding type (e.g. of patient names, social security numbers, postal codes, health institution names, health personnel names and locations). \
\\par\
B.  Exact Match and Tag - In this step, the focus is on processing the unigram created from the free-text notes. Based on all the local and external dictionaries, a combined dictionary is  created where one can look up words and get their corresponding type(s). In order to tag non-textual symbols such, e.g. dates, phone numbers and social security numbers, regular expressions are used.\
\\par\
C. Approximate Match and Tag - Patient records may contain erroneously spelled words, and in many cases they might be only slightly incorrectly spelled. Levenstein distance is used; by going through untagged words in the unigram and allowing an edit distance of 1, candidate misspellings can be found by relating them to the combined dictionary.\
\\par\
D1. Resolve Tagged Words - The words in the unigram should be replaced only if it is an identifying name (e.g. person name). When words have multiple type tags, they have to be replaced or removed if one or more of the tags are of the identifying type.\
\\par\
D2. Handle Untagged Words - Untagged words in the unigram needs to be investigated manually by the local clinician for tagging, the result of this investigation could be additional entries for the local or external dictionaries.\
\\par\
E. Final Result - The final result contains patient record text with identifying entities replaced by pseudonyms. In order to ensure an acceptable level of anonymization the result must be validated according to the requirements that motivated the anonymization in the first place. \
\
\\subsection\{State-of-the-art Anonymization of Medical Records Using an Iterative Machine Learning Framework\\cite\{paper3\}\}\
The authors have developed a de-identification model that can successfully remove personal health information (PHI) from discharge records to make them conform to the guidelines of the HIPAA.\
\\par\
Feature set Built:\
Authors have used feature set of 5 different categories described below:\
Word level features: Word level features like Word Capitalization, punctuation marks, digits, Roman/Arabic numbers and other common word level feature of phone numbers, fax, age and ID\'92s etc \
Frequency Information: Gathered frequency of tokens from the Internet. Used features like frequency of the token, the ratio of the token\'92s capitalized and lowercase occurrences, the ratio of capitalized and sentence beginning frequencies of the token.\
Offline Dictionaries: 5 different dictionaries collected from the Internet.\
Contextual Information: Sentence position, closest Heading to the section and several other features.\
Phrasal Information: a forecasted class of several preceding words and common suffixes etc.	\
\\par\
Authors have trained 3 different classifiers that used 3 different contextual features and used a voting based mechanism to decide if the word belonged to N.E.R. If any 2 classifiers have predicted the same label, the word is assigned that tag otherwise it\'92s not considered NER.\
\\par\
Authors have used an iterative approach in the following way:\
In the first iteration, they have collected several named entities from the headings and used these entities as an extra dictionary. They train the classifiers and collected all the texts tagged as entities from Step 1.\
They used all the texts tagged as entities and added this dictionary set as an extra dictionary for step 2.\
\
\\subsection\{Anonymizing and Sharing Medical Text Records\\cite\{paper4\}\}\
There are three categories in medical texts: (i)explicit identifiers (EID) which includes patient name, phone number, and social security number, which can be used to directly identify an individual (ii)quasi-identifier (QID) which includes date of birth, admission/discharge date, hospital, and zip code, (iii)Health and medical details (HMDs) such as symptoms, test results, disease, and medications.\
\
\\par\
To extract these categories, a three step approach will be used.\
Step 1: The feature extractor: It will split the document into terms. Each term will include local features such as term length, part-of-speech, etc; global features such as the position of the term in the document. (e.g., header, body text, heading, etc.); external features regarding such as whether the term belongs to a proper noun list, belongs to a medical concept lexicon, etc\
\\par\
Step 2: Base classifiers: Use multiple classifiers independent of each other from the features that are extracted. Goal of the classifiers are to match the terms to EID, QID, HMD, or irrelevant categories\
\\par\
Step 3: Combine the results of independent classifiers to get final tags of the words\
\
\
\\section\{Experimental Design:\}\
\\subsection\{Dataset Description:\}\
The data is provided in Brat and XML formats. Training: 500 clinical cases (released). Development: 250 clinical case (scheduled release - April 4) .Test set with the background set is composed of at least 2,500 clinical case (scheduled release -\
April 29)\
Since the actual test data will be made available on April 29, for building our model and evaluation we will\
put 100 clinical cases aside each from the given training and development data.\
New data distribution:\
1. Training: 400 clinical cases\
2. Development: 150 clinical cases\
3. Testing: 200 clinical cases\
\
Challenges and motivation:\
1. Most of the research in medical domain is currently being done for English text, so doing the\
same for a low-resource language will be challenging but much needed.\
2. The need to share medical records is there to carry out research in the medical domain, so\
developing a system to automatically anonymize medical records will be of great help\
Note: We went through the task page and also discussed the project idea with few PhD fellows. We\
found it interesting as well as challenging and would like to take it up as our project for the class.\
Link to the dataset is found here \\cite\{DatasetLink\}.\
\
\\subsection\{Evaluation Metric:\}\
\
1. Entity recognition on the data:\
For this task evaluation will be entity-based. This task will require to compare the system output\
with the beginning and end locations of each PHI entity tag, as well as detecting correctly the\
annotation type.\
\\par\
2. Detect sensitive spans:\
For this task evaluation will be span-based, by just evaluating whether spans belonging to\
sensitive phrases are detected correctly. This boils down to a classification of spans, where\
systems try to obfuscate spans that contain sensitive PHI expressions.\
\\par\
For both sub-tasks, the metric used will be micro-averaged precision, recall, and balanced F-score:\
1. Precision (P) = true positives/(true positives + false positives)\
2. Recall (R) = true positives/(true positives + false negatives)\
3. F-score (F1) = 2*((P*R)/(P+R))\
\\par\
Higher F-scores are better for the above tasks.\
\\par\
Task 2 is a subsequent task after NER (Task 1). Task 2 can be taken up only after successful completion of Task 1.\
\
\
\\subsection\{Simple Baseline:\}\
\
\\subsubsection\{Preprocessing the data:\}\
\\par\
Step 1 - Parse all the labels in the training data from all the files to identify (word,tag) tuples:\
\
Our dataset is in Spanish language. We use the labelled data of the training set to identify the (word,tag) tuples. For example, if one of the labels in the training set contains (Ernesto, \'92NOMBRE\\_SUJETO\\_ASISTENCIA\'92) as one of the gold standard label for a file, we add (Ernesto,\'92NOMBRE\\_SUJETO\\_ASISTENCIA\'92) to the dictionary D. We populate the dictionary D using the approach mentioned above for all the files.\
\
\\par\
Step 2 - Parse the text from the training data from all the files to identify the list L of (identifier,word) tuples:\
\
We then parse all the files one by one for a second time. In each file, We search for all the keywords from the dictionary D. For each keyword found from the dictionary D, we search if \'91:\'92 is the character that appears before the keyword.If we find \'91:\'92 before the keyword, we label the word before \'91:\'92 as an Identifier.For example, if we find \'91Nombre: Ernesto\'92 in the running text, we add (\'91Nombre\'92) as an Identifier.  After the end of second pass of the text files, we get all the (Identifier,word)  tuples from all the files. From D, we have a mapping of (word,tag) and from the second pass of reading all the files, we have mappings of the form (Identifier, word). Using the transitive relation, we create a precomputed dictionary of ( Identifier, tag ). \
\\par\
Example:\
\
	Precomputed dictionary D looks something like:\
	\{ 'Nombre': 'NOMBRE_SUJETO_ASISTENCIA', \'85\'85...\}\
\
	After reading all the files second time, We get, L = [ (\'91Nombre\'92,\'92Ernesto\'92 ),....] (\'91Nombre\'92,\'92Ernesto\'92 ) is one of the (Identifier,word) tuple from the List L.\
\\par\
Step 3 - Combine the tuples from Dictionary D and List L to generate (identifier,tag) tuples that forms the basis for our baseline model:\
\
We use combine Dictionary D and the list L to generate the (identifier,tag) tuples. For example,(\'91Nombre\'92,\'92Ernesto\'92 ) is one of the (Identifier,word) tuple. All such tuples are stored as a dictionary on the disk and these tuples form the basis of our baseline model.\
\\par\
Step 4 - Creating the tags for the baseline model:\
\
We use regex expressions to find the words of the form word1:word2 from the training/validation files. For each such tuples found, we see if the word1 is a key from the precomputed Dictionary D1. If it is a key from the Dictionary D1, we label the word2 with the tag corresponding to the word1 from Dictionary D1.  This simple baseline does not involve any machine learning. It relies on  two things 1. Finding the : separated words from the running text 2. Assigning the tag to the words using the precomputed Dictionary D1.\
	\
	\\par\
	\\subsubsection\{Results for our baseline model:\}\
\
The results for the baseline model are presented in the table \\ref\{baseline\}\
\
\\begin \{table\}[H]\
\
\\begin\{center\}\
\\begin\{tabular\}\{ |c|c|c|c| \} \
 \\hline\
Classifier  & Precision &  Recall &  F1Score  \\\\ \
 \\hline\
Training Set & .634 & ..438 & \\textbf\{.518\} \\\\\
\\hline\
Validation Set & .629 & .425 & .507  \\\\\
\\hline\
Test set & .628 & .432 & .506  \\\\ \
\\hline\
\\end\{tabular\}\
\\caption\{Baseline Scores\} \\label\{baseline\}\
\\end\{center\}\
\\end\{table\}\
\\vspace\{-2em\}\
\
\\section \{Published Baseline\}\
\\par\
We have built a baseline model simulating the kind of work implemented in the paper on \'91Identifying Personal Health Information Using Support Vector Machines\'92 \\cite\{paper5\}. Since our dataset is in Spanish which is a low-resource language, we couldn\'92t implement some features because of the scarcity of data for this particular locale. We intend to include more features based off of Spanish as an extension in Milestone 4. We have tried to gather as many token-level features as we could find for Spanish in this milestone. \
\\par\
Our feature set is as follows:\
1. Word length \
2. Whether word contains 1 digit \
3. Whether word contains 2 digits \
4. Whether word contains 3 digits \
5. Whether word contains 5 digits \
6. Whether word contains 6 digits \
7. Whether word contains 7 digits \
8. Whether word contains 9 digits \
9. Whether there\'92s an uppercase letter \
10. Whether the word contains punctuations \
11. Whether the word is Roman \
12. Whether the word contains \'91edad\'92 or \'91a\'f1os\'92 \
13. Whether word is all uppercase \
14. Whether word is all lowercase \
15. Whether an apostrophe is present in the word \
16. Whether a dash is present in the word \
17. Whether the word contains \'91fax\'92 \
\\par\
\\par\
For each word we look at the window [-1,0,1,2] and create feature vector including the features of these words in the context of the target word. \
We have picked these particular features because: \uc0\u9679  Digits: \
		
\f1 \uc0\u9675 
\f0  \'a0Spanish phone numbers contain 9 digits and their format varies. It may come in 3 3 3, 3-3-3, 9, 2 3 2 2, 3 2 2 2, 3 6, 2 7 \uc0\u8232 \
		
\f1 \uc0\u9675 
\f0  \'a0That is why we created a feature to capture all these different formats \uc0\u8232 \
		\uc0\u9679  \'a0Upper/Lower cases To extract words that are part of names or addresses \u8232 \
		\uc0\u9679  \'a0Edad/Anos to get values associated with age \u8232 \
\\newline\
\uc0\u9679  To distinguish between phone and fax numbers Experiments with features: \
We have a total of 401 training documents, 193 dev documents and 156 test documents and we tokenized the files into sentences who counts are mentioned as follows - \
train sentences len equal to 8300 (Total (401 docs)) dev sentences len equal to 4048 (Total (193 docs)) test sentences len equal to 3231 (Total (156 docs)) .\
\
Our Model used is LinearSVC  and the results are presented in table[\\ref\{publishedpaper\}]. As we can see, we have achieved a significant improvement in results by adding these features. Our F1 score jumped to 86\\% after adding these new features. Although the authors achieved a score of 89\\% on their original paper. WE cannot compare our results with them as our data is in Spanish while there data is in English.\
\
\
\\begin \{table\}[H]\
\
\\begin\{center\}\
\\begin\{tabular\}\{ |c|c|c|c| \} \
 \\hline\
Classifier  & Precision &  Recall &  F1Score  \\\\ \
 \\hline\
Baseline & .628 & .423 & .506 \\\\\
\\hline\
LinearSVC & .891 & .833 & .861  \\\\\
\
\\hline\
\\end\{tabular\}\
\\caption\{Baseline Scores\} \\label\{publishedpaper\}\
\\end\{center\}\
\\end\{table\}\
\\vspace\{-2em\}\
\\section\{Extension  1\}\
For our extension 1, we have extended the model submitted for Milestone 3. For Milestone 3 , we have trained a SVM with the following features:0. word \
1. Word length \
2. Whether word contains 1 digit \
3. Whether word contains 2 digits \
4. Whether word contains 3 digits \
5. Whether word contains 5 digits \
6. Whether word contains 6 digits \
7. Whether word contains 7 digits \
8. Whether word contains 9 digits \
9. Whether there\'92s an uppercase letter \
10. Whether the word contains punctuations \
11. Whether the word is Roman \
12. Whether the word contains \'91edad\'92 or \'91a\'f1os\'92 \
13. Whether word is all uppercase \
14. Whether word is all lowercase \
15. Whether an apostrophe is present in the word \
16. Whether a dash is present in the word \
17. Whether the word contains \'91fax\'92 \
\\newline\
We have added the extension 1 in two phases. In the first phase, we have added dictionary lookup features.In the second phase, we have added regex features on top of phase 1.\
\
\\subsection\{Word level features + Dictionary lookup features\}\
As an extension of Milestone 3, we have added dictionary level features for Milestone4. Milestone 3 had word level features and Milestone 4 classifiers are trained on word level features + dictionary lookup features.\
We have taken two dictionaries to built the feature set. Our first dictionary\\cite\{dict1\} is a dictionary of spanish names along with their mean age and frequency. Our second dictionary is a list of spanish location names\\cite\{dict2\} taken from wikipedia. Thereby, we have added dictionary level features to our feature set. Using the two dictionaries mentioned here, we have added the following features on top of the features already mentioned.\
18. Is word a Male Name?\
19. Male Avg. Age\
20. Male Name Avg. Frequency\
21. Is word a Female Name?\
22. Female Name Avg. Age\
23. Female Name Avg. Frequency\
24. Is Word a location?\
\\subsection\{Results:\}\
We have trained various classifiers on the train/dev/test results and the results for the training data are presented in  the table ~\\ref\{ext1train1\}, the results for the validation set are presented in the table  ~\\ref\{ext1dev1\} and the results for the test set are presented in the table  ~\\ref\{ext1test1\}.\
\\subsection\{Word level features + Dictionary lookup features + regex features\}\
TODO: ILAYDA: Add the regex features and their analysis.\
\\subsubsection\{Results:\}\
\
\\section\{Extension 2\}\
\\section\{Analysis\}\
\
\\begin \{table\}[H]\
\
\\begin\{center\}\
\\begin\{tabular\}\{ |c|c|c|c| \} \
 \\hline\
Classifier  & Precision &  Recall &  F1Score  \\\\ \
 \\hline\
LinearSVC(C=1) & .979 & .941 & \\textbf\{.959\} \\\\\
\\hline\
Perceptron(alpha=0.0001) & .661 & .669 & .665  \\\\\
\\hline\
PassiveAggressiveClassifier() & .840 & .725 & .779  \\\\ \
\\hline\
DecisionTreeClassifier() & .984 & .944 & .964  \\\\ \
\\hline\
RandomForestClasifier() & .978 & .936 & .956 \\\\ \
\\hline\
LogisticRegression() & .911 & .838 & .873  \\\\ \
\\hline\
RandomForestClassifier(n_estimators=25) & .984 & .941  & .962 \\\\ \
\\hline\
LinearSVC(C=0.0001) & .702 & .382 & .495 \\\\ \
\\hline\
LinearSVC(C=10) & .975 & .942 & .958  \\\\ \
\\hline\
\\end\{tabular\}\
\\caption\{Word level Features + Dictionary Lookup features on Train Data\} \\label\{ext1train1\}\
\\end\{center\}\
\\end\{table\}\
\\vspace\{-2em\}\
\
\
\\begin \{table\}[H]\
\
\\begin\{center\}\
\\begin\{tabular\}\{ |c|c|c|c| \} \
 \\hline\
Classifier  & Precision &  Recall &  F1Score  \\\\ \
 \\hline\
LinearSVC(C=1) & .886 & .824 & \\textbf\{.854\} \\\\\
\\hline\
Perceptron(alpha=0.0001) & .650 & .643 & .646  \\\\\
\\hline\
PassiveAggressiveClassifier() & .832 & .696 & .758  \\\\ \
\\hline\
DecisionTreeClassifier() & .836 & .787 & .811  \\\\ \
\\hline\
RandomForestClasifier() & .870 & .792 & .829 \\\\ \
\\hline\
LogisticRegression() & .877 & .782 & .827  \\\\ \
\\hline\
RandomForestClassifier(n_estimators=25) & .886 & .796  & .838 \\\\ \
\\hline\
LinearSVC(C=0.0001) & .705 & .371 & .486 \\\\ \
\\hline\
LinearSVC(C=10) & .877 & .828 & .852  \\\\ \
\\hline\
\\end\{tabular\}\
\\caption\{Word level Features + Dictionary Lookup features on Validation Data\} \\label\{ext1dev1\}\
\\end\{center\}\
\\end\{table\}\
\\vspace\{-2em\}\
\\begin \{table\}[H]\
\
\\begin\{center\}\
\\begin\{tabular\}\{ |c|c|c|c| \} \
 \\hline\
Classifier  & Precision &  Recall &  F1Score  \\\\ \
 \\hline\
LinearSVC(C=1) & .889 & .837 & \\textbf\{.862\} \\\\\
\\hline\
Perceptron(alpha=0.0001) & .643 & .663 & .653  \\\\\
\\hline\
PassiveAggressiveClassifier() & .829 & .710 & .765  \\\\ \
\\hline\
DecisionTreeClassifier() & .835 & .785 & .809  \\\\ \
\\hline\
RandomForestClasifier() & .879 & .803 & .839 \\\\ \
\\hline\
LogisticRegression() & .874 & .788 & .829  \\\\ \
\\hline\
RandomForestClassifier(n_estimators=25) & .888 & .802  & .843 \\\\ \
\\hline\
LinearSVC(C=0.0001) & .701 & .370 & .485 \\\\ \
\\hline\
LinearSVC(C=10) & .871 & .833 & .852  \\\\ \
\\hline\
\\end\{tabular\}\
\\caption\{Word level Features + Dictionary Lookup features on Test Data\} \\label\{ext1test1\}\
\\end\{center\}\
\\end\{table\}\
\\vspace\{-2em\}\
\
\
\
\\section\{Recommendations and future work\}\
\
\
\\section\{Conclusion\}\
\
\
\\bibliographystyle\{plain\}\
\\bibliography\{references\}\
\\end\{document\}\
}