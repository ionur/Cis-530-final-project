{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "milestone-4-ext1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "z3R7nQYPkGsz",
        "colab_type": "code",
        "outputId": "bb005e16-f080-443c-c26f-fad6514f6577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount your google drive. \n",
        "# Use this to save your PyTorch model for submission\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!mkdir /content/gdrive/Team\\ Drives/cis530\n",
        "#Test drive access. \n",
        "#You should have a test.txt in your Google drive\n",
        "with open('/content/gdrive/Team Drives/cis530/test.txt', 'w') as f:\n",
        "  f.write('This is a test file!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘/content/gdrive/Team Drives/cis530’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sIaKrm1bQdHl",
        "colab_type": "code",
        "outputId": "76ef1612-f35d-4e73-e0c1-32638f90cd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import conll2002\n",
        "from nltk.corpus import cess_esp as cess\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import pickle\n",
        "!pip3 install wordfreq\n",
        "from collections import OrderedDict \n",
        "from wordfreq import zipf_frequency\n",
        "!pip3 install pandas\n",
        "import pandas as pd\n",
        "import string\n",
        "import datetime\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
            "Requirement already satisfied: regex<=2018.02.21,>=2017.07.11 in /usr/local/lib/python3.6/dist-packages (from wordfreq) (2018.1.10)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from wordfreq) (0.5.6)\n",
            "Requirement already satisfied: langcodes>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from wordfreq) (1.4.1)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.6/dist-packages (from langcodes>=1.4.1->wordfreq) (0.7.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IFey8Jisw-67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the names from .csv file to python\n",
        "names_male = pd.read_csv('/content/gdrive/Team Drives/cis530/names/male_names.csv',)\n",
        "names_female = pd.read_csv('/content/gdrive/Team Drives/cis530/names/female_names.csv')\n",
        "\n",
        "#Create the male dict\n",
        "male_dict={}\n",
        "\n",
        "for index, row in names_male.iterrows():\n",
        "    _key = row['name']\n",
        "    _val1 = row['mean_age']\n",
        "    _val2 = row['frequency']\n",
        "    male_dict[_key] = (_val1,_val2)\n",
        "    \n",
        "    \n",
        "#Create the female dict\n",
        "female_dict={}\n",
        "\n",
        "for index, row in names_female.iterrows():\n",
        "    _key = row['name']\n",
        "    _val1 = row['mean_age']\n",
        "    _val2 = row['frequency']\n",
        "    female_dict[_key] = (_val1,_val2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PFOnEx20BxJx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Locations Dictionary\n",
        "#Taken from https://en.wikipedia.org/wiki/List_of_municipalities_of_Spain\n",
        "locations=['Madrid','Barcelona','Valencia','Seville','Zaragoza','Málaga','Murcia','Palma',\n",
        "        'Las Palmas de Gran Canaria','Bilbao','Alicante','Córdoba','Valladolid',\n",
        "        'Vigo','Gijón','L Hospitalet de Llobregat','A Coruña','Vitoria-Gasteiz',\n",
        "        'Granada','Elche','Oviedo','Badalona','Cartagena','Terrassa','Jerez de la Frontera']\n",
        "\n",
        "        #'Sabadell','Santa Cruz de Tenerife','Móstoles','Alcalá de Henares','Pamplona','Fuenlabrada',\n",
        "        #'Almería','Leganés','Donostia-San Sebastián','Burgos','Santander',\n",
        "        #'Castellón de la Plana','Getafe','Albacete',\n",
        "        #'Alcorcón','Logroño','San Cristóbal de La Laguna','Badajoz','Salamanca','Huelva','Lleida',\n",
        "        #'Marbella','Tarragona','León','Cádiz','Tineo','Baza','Alcántara','Don Benito',\n",
        "        #'Piedrabuena','Alhambra','Sabiñánigo','Montoro','Torrelavega','Guadalajara','Palencia',\n",
        "        #'Vic–Manlleu','Ourense']\n",
        "\n",
        "locations_set = set()\n",
        "\n",
        "for location in locations:\n",
        "  if location.upper() not in locations_set:\n",
        "    locations_set.add(location.upper())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8OT91hjTc_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def isApostrophePresent(word):\n",
        "    if \"'\" in word:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  \n",
        "def isDashPresent(word):\n",
        "    if \"-\" in word:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def oneDigit(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 1:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "def twoDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 2:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def threeDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 3:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def fiveDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 5:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def sixDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 6:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def sevenDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 7:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def nineDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 9:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def fax(word):\n",
        "    if \"fax\" in word.lower():\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "def isAge(word):\n",
        "  if \"edad\" in word.lower() or \"años\" in word.lower():\n",
        "    return True\n",
        "  \n",
        "  return False\n",
        "\n",
        "    \n",
        "def hasPunctuation(word):\n",
        "  #This might return a lot of false positives\n",
        "  \n",
        "  for letter in word:\n",
        "    if( letter  in string.punctuation):\n",
        "      return True\n",
        "    \n",
        "  return False\n",
        "\n",
        "\n",
        "def isRoman(word):\n",
        "  #This might return a lot of false positives\n",
        "  romans = ['I','V','M','L','X','D','C']\n",
        "  \n",
        "  for letter in word:\n",
        "    if( letter not in romans):\n",
        "      return False\n",
        "    \n",
        "  return True\n",
        "\n",
        "def isDigit(word):\n",
        "  for letter in word:\n",
        "    if not letter.isdigit():\n",
        "      return False\n",
        "    \n",
        "  return True\n",
        "\n",
        "def maleFeatures(word):\n",
        "   if word in male_dict:\n",
        "    return male_dict[word]\n",
        "   else:\n",
        "    return (0,0)\n",
        "    \n",
        "\n",
        "def femaleFeatures(word):\n",
        "   if word in female_dict:\n",
        "    return female_dict[word]\n",
        "   else:\n",
        "    return (0,0)\n",
        "  \n",
        "def isLocation(word):\n",
        "  if word.upper() in locations_set:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kFRisyy4Tdno",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getfeats(word, postag, o):\n",
        "    \"\"\" This takes the word in question and\n",
        "    the offset with respect to the instance\n",
        "    word \"\"\"\n",
        "    o = str(o)\n",
        "    _male = maleFeatures(word)\n",
        "    _female = femaleFeatures(word)\n",
        "    _isMale = False\n",
        "    _isFemale = False\n",
        "    \n",
        "    if( _male[0] != 0 or _male[1] != 0) :\n",
        "        _isMale = True\n",
        "        \n",
        "    if( _female[0] != 0 or _female[1] != 0) :\n",
        "        _isFemale = True\n",
        "    \n",
        "    features = [\n",
        "        (o + 'word', word), #0\n",
        "        # TODO: add more features here.\n",
        "        \n",
        "        (o + 'word.len', len(word) ), #1\n",
        "        (o + 'oneDigit', oneDigit(word)), #2\n",
        "        (o + 'twoDigits', twoDigits(word)), #3\n",
        "        (o + 'threeDigits', threeDigits(word)), #4\n",
        "        (o + 'fiveDigits', fiveDigits(word)), #5\n",
        "        (o + 'sixDigits', sixDigits(word)), #6\n",
        "        (o + 'sevenDigits', sevenDigits(word)), #7\n",
        "        (o + 'nineDigits', nineDigits(word)), #8\n",
        "        (o + 'word.isupper', any(letter.isupper() for letter in word)), #9\n",
        "        (o + 'hasPunctuation', hasPunctuation(word)), #10\n",
        "        (o + 'isRoman', isRoman(word)), #11\n",
        "        (o + 'age', isAge(word)), #12\n",
        "        (o + 'isupper', word.isupper()), #13\n",
        "        (o + 'islower', word.islower()), #14\n",
        "        (o + 'isApostrophePresent', isApostrophePresent(word)), #15\n",
        "        (o + 'isDashPresent', isDashPresent(word)), #16\n",
        "        (o + 'fax', fax(word)), #17\n",
        "   \n",
        "       \n",
        "        (o + 'word.wordfreq', zipf_frequency(word, 'es') ), #23\n",
        "        (o + 'word_count_en', zipf_frequency(word, 'en')) , #24\n",
        "        (o + 'isMale', _isMale ), #16\n",
        "         (o + 'maleAvgAge', _male[0] ), #17\n",
        "         (o + 'maleAvgFreq', _male[1]), #18\n",
        "        (o + 'isFemale', not _isMale ), #19\n",
        "         (o + 'femaleAvgAge', _female[0] ), #20\n",
        "         (o + 'femaleAvgFreq', _female[1] ), #21\n",
        "        (o + 'isLocation', isLocation(word)) #22\n",
        "        \n",
        "    ]\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWE_ZtfUTgQJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "def word2features(sent, i):\n",
        "    global count\n",
        "    \"\"\" The function generates all features\n",
        "    for the word at position i in the\n",
        "    sentence.\"\"\"\n",
        "    features = []\n",
        "    # the window around the token\n",
        "    featlist = [('bias', 1.0)]\n",
        "    features.extend(featlist)\n",
        "  \n",
        "    #print('Processed :',count,' sentences')\n",
        "    count = count+1\n",
        "    for o in [-1,0,1,2]:\n",
        "        if i+o >= 0 and i+o < len(sent):\n",
        "            word = sent[i+o][0]\n",
        "#             postag = sent[i+o][1]\n",
        "            postag = \"unknown\"\n",
        "            featlist = getfeats(word, postag, o)\n",
        "            features.extend(featlist)\n",
        "        elif i+o<0:\n",
        "            featlist = [('BOS', 1)]\n",
        "            features.extend(featlist)\n",
        "        else:\n",
        "            featlist = [('EOS', 1)]\n",
        "            features.extend(featlist)    \n",
        "    return dict(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U0Z7S6Ig-OS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create an .ann format for predictions\n",
        "def createAnnFormat(preprocess_dict, y_pred, pathRead, pathOut):   \n",
        "  \n",
        "  \n",
        "    j = 0\n",
        "    for docId, v in preprocess_dict.items():\n",
        "        #print('Generating .ann for ',docId)\n",
        "#         #Get the y_pred\n",
        "#         X = preprocess_dict[docId]        \n",
        "        #make predictions and store the results with their tags\n",
        "        txt_start_end = \"\"\n",
        "        tmp_tags=[]\n",
        "        curr_tag = \"\"\n",
        "        \n",
        "        for sent in v:\n",
        "          for item in sent:\n",
        "            word = item[0]\n",
        "            word_start= item[2]\n",
        "            word_end = word_start + len(word) - 1\n",
        "            pred = y_pred[j]   \n",
        "\n",
        "            #if prediction is a beginning, add the tag to the tag list\n",
        "            if pred[0:2] == \"B-\":\n",
        "              if txt_start_end != \"\":\n",
        "                tmp_tags.append((txt_start_end,curr_tag))\n",
        "                txt_start_end= \"\"\n",
        "              curr_tag= pred[2:]\n",
        "              txt_start_end +=\"\"+str(word_start)+\"-\"+str(word_end)+\",\"\n",
        "            #if it is a contuniation keep adding \n",
        "            elif pred[0:2] == \"I-\":\n",
        "              txt_start_end +=\"\"+str(word_start)+\"-\"+str(word_end)+\",\"\n",
        "            j += 1\n",
        "        if txt_start_end != \"\":\n",
        "          tmp_tags.append((txt_start_end,curr_tag))\n",
        "          txt_start_end= \"\"  \n",
        "        #we might be missing some chars in between values, so parse the data and \n",
        "        #get sentence matching that                        \n",
        "        with open(pathRead+docId+\".txt\", \"r\") as f:                                                   \n",
        "          complete_doc = f.read()       \n",
        "\n",
        "        #now for all the text, get their exact match and create tags\n",
        "        tags = []\n",
        "        t_count = 1                        \n",
        "        for i, i_tag in tmp_tags:\n",
        "          splits = i.split(\",\")[:-1]\n",
        "          beginning_split = splits[0].split(\"-\")                 \n",
        "          real_start = int(beginning_split[0])                      \n",
        "          if len(splits) ==1:                  \n",
        "            real_end = int(beginning_split[1])+1\n",
        "          else:\n",
        "            end_split = splits[len(splits)-1].split(\"-\")\n",
        "            real_end = int(end_split[1])+1\n",
        "          text_appearing = complete_doc[real_start:real_end].split(\"\\n\")[0]\n",
        "          tags.append(\"T\"+str(t_count)+\"\\t\"+i_tag+\" \"+str(real_start)+\" \"+str(real_end)+\"\\t\"+ text_appearing)\n",
        "          t_count += 1\n",
        "        #now output all these\n",
        "        with open(pathOut+docId+\".ann\", \"w\") as out:\n",
        "          for i in tags:\n",
        "            out.write(i+\"\\n\")                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mgrWfug75-T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Evaluation from medotask evaluate.py script\n",
        "import os\n",
        "os.chdir('/content/gdrive/Team Drives/cis530/Milestone3_Code')\n",
        "\n",
        "def evaluate_model(model):\n",
        "  print('Generating Training results for the model',model)\n",
        "\n",
        "\n",
        "  !python \"evaluate.py\" \"brat\" \"ner\" \"/content/gdrive/Team Drives/cis530/raw_system_data/train/gold/\" \"/content/gdrive/Team Drives/cis530/raw_system_data/train/system/\" \n",
        "\n",
        "\n",
        "  !python \"evaluate.py\" \"brat\" \"ner\" \"/content/gdrive/Team Drives/cis530/raw_system_data/dev/gold/\" \"/content/gdrive/Team Drives/cis530/raw_system_data/dev/system/\" \n",
        "\n",
        "\n",
        "  !python \"evaluate.py\" \"brat\" \"ner\" \"/content/gdrive/Team Drives/cis530/raw_system_data/test/gold/\" \"/content/gdrive/Team Drives/cis530/raw_system_data/test/system/\" \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HIyr1WKDTidh",
        "colab_type": "code",
        "outputId": "2cf42f95-b30b-4a6a-f94c-e3c474d4eab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "    # Load the training data\n",
        "    file = open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/train_word_ner_startidx_dict.pickle','rb')\n",
        "    train_dict = pickle.load(file)\n",
        "    \n",
        "\n",
        "    train_dict = OrderedDict(train_dict)\n",
        "    \n",
        "    file.close()\n",
        "    \n",
        "    train_sents = []\n",
        "    \n",
        "    for k, v in train_dict.items():\n",
        "      train_sents.extend(v)\n",
        "\n",
        "    print(\"train_sents len= \", len(train_sents))\n",
        "\n",
        "    file = open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/dev_word_ner_startidx_dict.pickle','rb')\n",
        "    dev_dict = pickle.load(file)\n",
        "    \n",
        "\n",
        "    dev_dict = OrderedDict(dev_dict)\n",
        "    file.close()\n",
        "    \n",
        "    dev_sents = []\n",
        "    for k, v in dev_dict.items():\n",
        "      dev_sents.extend(v)\n",
        "    \n",
        "    print(\"dev_sents len= \", len(dev_sents))\n",
        "      \n",
        "    file = open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/test_word_ner_startidx_dict.pickle','rb')\n",
        "    test_dict = pickle.load(file)\n",
        "    \n",
        "  \n",
        "    test_dict = OrderedDict(test_dict)\n",
        "    file.close()\n",
        "    \n",
        "    test_sents = []\n",
        "    for k, v in test_dict.items():\n",
        "      test_sents.extend(v)\n",
        "    \n",
        "    print(\"test_sents len= \", len(test_sents))\n",
        "    \n",
        "    train_feats = []\n",
        "    train_labels = []\n",
        "\n",
        "    print('Started preparing the features',datetime.datetime.now())\n",
        "    for sent in train_sents:\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent,i)\n",
        "            train_feats.append(feats)\n",
        "            train_labels.append(sent[i][1])\n",
        "\n",
        "    vectorizer = DictVectorizer()\n",
        "    X_train = vectorizer.fit_transform(train_feats)\n",
        "    print('Finished preparing the features',datetime.datetime.now())\n",
        "\n",
        "#     # TODO: play with other models\n",
        "#     # model = Perceptron(verbose=1)\n",
        "#     # model = MultinomialNB(alpha=0.01)\n",
        "#     #model = PassiveAggressiveClassifier(C=1.0, fit_intercept=True, early_stopping=False, loss='hinge', average=True, random_state=99)\n",
        "#     # model = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, fit_intercept=True, random_state=99)\n",
        "#     #model = LinearSVC(C=10)\n",
        "    models = [LogisticRegression()]\n",
        "  \n",
        "    for model in models:\n",
        "      print('Training the model',datetime.datetime.now())\n",
        "      model.fit(X_train, train_labels)\n",
        "      print('Trained the model',datetime.datetime.now())\n",
        "\n",
        "      #Training Data\n",
        "      y_train_pred = model.predict(X_train)\n",
        "    \n",
        "      train_pathRead = \"/content/gdrive/Team Drives/cis530/raw_system_data/train/system/\"\n",
        "      train_pathOut = \"/content/gdrive/Team Drives/cis530/raw_system_data/train/system/\"\n",
        "    \n",
        "      #create an .ann format for predictions\n",
        "      createAnnFormat(train_dict, y_train_pred,train_pathRead, train_pathOut)\n",
        "   \n",
        "         \n",
        "      #Dev Data\n",
        "      dev_feats = []\n",
        "      dev_labels = []\n",
        "\n",
        "      for sent in dev_sents:\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent,i)\n",
        "            dev_feats.append(feats)\n",
        "            dev_labels.append(sent[i][1])\n",
        "\n",
        "      X_dev = vectorizer.transform(dev_feats)\n",
        "      y_dev_pred = model.predict(X_dev)\n",
        "\n",
        "    \n",
        "      dev_pathRead = \"/content/gdrive/Team Drives/cis530/raw_system_data/dev/system/\"\n",
        "      dev_pathOut = \"/content/gdrive/Team Drives/cis530/raw_system_data/dev/system/\"\n",
        "\n",
        "      createAnnFormat(dev_dict, y_dev_pred,dev_pathRead, dev_pathOut)\n",
        "\n",
        "      #Test Data\n",
        "      test_feats = []\n",
        "      test_labels = []\n",
        "\n",
        "      # switch to test_sents for your final results\n",
        "      for sent in test_sents:\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent,i)\n",
        "            test_feats.append(feats)\n",
        "            test_labels.append(sent[i][1])\n",
        "\n",
        "      X_test = vectorizer.transform(test_feats)\n",
        "      y_test_pred = model.predict(X_test)\n",
        "\n",
        "        \n",
        "      test_pathRead = \"/content/gdrive/Team Drives/cis530/raw_system_data/test/system/\"\n",
        "      test_pathOut = \"/content/gdrive/Team Drives/cis530/raw_system_data/test/system/\"\n",
        "    \n",
        "      createAnnFormat(test_dict, y_test_pred,test_pathRead, test_pathOut)    \n",
        "      \n",
        "      \n",
        "      evaluate_model(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_sents len=  8300\n",
            "dev_sents len=  4048\n",
            "test_sents len=  3231\n",
            "Started preparing the features 2019-04-28 20:28:41.714272\n",
            "Finished preparing the features 2019-04-28 20:29:11.569954\n",
            "Training the model 2019-04-28 20:29:11.570848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "GZSzYX8O5rLu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}