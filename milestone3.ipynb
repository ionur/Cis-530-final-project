{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "milestone3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "z3R7nQYPkGsz",
        "colab_type": "code",
        "outputId": "acf4d3d0-ddc7-4e80-87ac-527a8de8b32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount your google drive. \n",
        "# Use this to save your PyTorch model for submission\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!mkdir /content/gdrive/Team\\ Drives/cis530\n",
        "#Test drive access. \n",
        "#You should have a test.txt in your Google drive\n",
        "with open('/content/gdrive/Team Drives/cis530/test.txt', 'w') as f:\n",
        "  f.write('This is a test file!')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "mkdir: cannot create directory ‘/content/gdrive/Team Drives/cis530’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sIaKrm1bQdHl",
        "colab_type": "code",
        "outputId": "4b7cd856-6281-4a55-a24c-11442ae85ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import conll2002\n",
        "from nltk.corpus import cess_esp as cess\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import pickle\n",
        "!pip3 install wordfreq\n",
        "from collections import OrderedDict \n",
        "from wordfreq import zipf_frequency\n",
        "!pip3 install pandas\n",
        "import pandas as pd\n",
        "import string"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
            "Requirement already satisfied: langcodes>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from wordfreq) (1.4.1)\n",
            "Requirement already satisfied: regex<=2018.02.21,>=2017.07.11 in /usr/local/lib/python3.6/dist-packages (from wordfreq) (2018.1.10)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from wordfreq) (0.5.6)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.6/dist-packages (from langcodes>=1.4.1->wordfreq) (0.7.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CA-Ft5sYPjZ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the names from .csv file to python\n",
        "names_male = pd.read_csv('/content/gdrive/Team Drives/cis530/names/male_names.csv')\n",
        "names_female = pd.read_csv('/content/gdrive/Team Drives/cis530/names/female_names.csv')\n",
        "index = names_female [ names_female['name'] == \"LUCY\" ].index.tolist()\n",
        "\n",
        "\n",
        "#Test the df \n",
        "#print( str( df.iloc[index]['name'].tolist()[0]))\n",
        "#print(df.iloc[index]['frequency'].tolist()[0])\n",
        "#print( df.iloc[index]['mean_age'].tolist()[0])\n",
        "#maleAvgAge('JUAN MARIANO')\n",
        "#maleAvgFreq('JUAN MARIANO')\n",
        "#print( 'female avg age:',femaleAvgAge('JUAN MARIANO'))\n",
        "#print('female avg freq',femaleAvgFreq('JUAN MARIANO'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8OT91hjTc_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def isApostrophePresent(word):\n",
        "    if \"'\" in word:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  \n",
        "def isDashPresent(word):\n",
        "    if \"-\" in word:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def oneDigit(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 1:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "def twoDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 2:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def threeDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 3:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def fiveDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 5:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def sixDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 6:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def sevenDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 7:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def nineDigits(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 9:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "def isPhoneNumber(word):\n",
        "    num = 0\n",
        "    for l in word:\n",
        "      if l.isdigit():\n",
        "        num +=1\n",
        "    if num == 10:\n",
        "      \n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "def fax(word):\n",
        "    if \"fax\" in word.lower():\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "def isAge(word):\n",
        "  if \"edad\" in word.lower() or \"anos\" in word.lower():\n",
        "    return True\n",
        "  \n",
        "  return False\n",
        "\n",
        "    \n",
        "def hasPunctuation(word):\n",
        "  #This might return a lot of false positives\n",
        "  \n",
        "  for letter in word:\n",
        "    if( letter  in string.punctuation):\n",
        "      return True\n",
        "    \n",
        "  return False\n",
        "\n",
        "\n",
        "def isRoman(word):\n",
        "  #This might return a lot of false positives\n",
        "  romans = ['I','V','M','L','X','D','C']\n",
        "  \n",
        "  for letter in word:\n",
        "    if( letter not in romans):\n",
        "      return False\n",
        "    \n",
        "  return True\n",
        "\n",
        "def isDigit(word):\n",
        "  for letter in word:\n",
        "    if not letter.isdigit():\n",
        "      return False\n",
        "    \n",
        "  return True\n",
        "\n",
        "def isMale(word):\n",
        "  index = names_male [ names_male['name'] == word.upper() ].index.tolist()\n",
        "  if len(index) > 0 :\n",
        "    return True\n",
        "  return False \n",
        "\n",
        "def isFemale(word):\n",
        "  index = names_female [ names_female['name'] == word.upper() ].index.tolist()\n",
        "  if len(index) > 0 :\n",
        "    return True\n",
        "  return False\n",
        "  \n",
        "def maleAvgAge(word):\n",
        "  if (isMale (word) ):\n",
        "    index = names_male [ names_male['name'] == word.upper() ].index.tolist()\n",
        "    return names_male.iloc[index]['mean_age'].tolist()[0]\n",
        "  else:\n",
        "    return 0\n",
        " \n",
        "def femaleAvgAge(word):\n",
        "  if (isFemale (word) ):\n",
        "    index = names_female [ names_female['name'] == word.upper() ].index.tolist()\n",
        "    return names_female.iloc[index]['mean_age'].tolist()[0]\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "def maleAvgFreq(word):\n",
        "  if (isMale (word) ):\n",
        "    index = names_male [ names_male['name'] == word.upper() ].index.tolist()\n",
        "    return names_male.iloc[index]['frequency'].tolist()[0]\n",
        "  else:\n",
        "    return 0\n",
        " \n",
        "def femaleAvgFreq(word):\n",
        "  if (isFemale (word) ):\n",
        "    index = names_female [ names_female['name'] == word.upper() ].index.tolist()\n",
        "    return names_female.iloc[index]['frequency'].tolist()[0]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kFRisyy4Tdno",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getfeats(word, postag, o):\n",
        "    \"\"\" This takes the word in question and\n",
        "    the offset with respect to the instance\n",
        "    word \"\"\"\n",
        "    o = str(o)\n",
        "    features = [\n",
        "        (o + 'word', word), #0\n",
        "        # TODO: add more features here.\n",
        "        \n",
        "        (o + 'word.len', len(word) ), #1\n",
        "        (o + 'oneDigit', oneDigit(word)), #2\n",
        "        (o + 'twoDigits', twoDigits(word)), #2\n",
        "        (o + 'threeDigits', threeDigits(word)), #2\n",
        "        (o + 'fiveDigits', fiveDigits(word)), #2\n",
        "        (o + 'sixDigits', sixDigits(word)), #2\n",
        "        (o + 'sevenDigits', sevenDigits(word)), #2\n",
        "        (o + 'nineDigits', nineDigits(word)), #2\n",
        "        (o + 'word.isupper', any(letter.isupper() for letter in word)), #3\n",
        "        (o + 'hasPunctuation', hasPunctuation(word)), #4\n",
        "        (o + 'isRoman', isRoman(word)), #5\n",
        "        (o + 'age', isAge(word)), #6\n",
        "        (o + 'isPhoneNumber', isPhoneNumber(word)),#7\n",
        "        (o + 'isupper', word.isupper()), #8\n",
        "        (o + 'islower', word.islower()), #9\n",
        "        (o + 'word.wordfreq', zipf_frequency(word, 'es') ), #10\n",
        "        (o + 'isApostrophePresent', isApostrophePresent(word)), #11\n",
        "        (o + 'isDashPresent', isDashPresent(word)), #12\n",
        "        (o + 'fax', fax(word)), #13\n",
        "        (o + 'isDigit', isDigit(word)), #14\n",
        "        (o + 'word_count_en', zipf_frequency(word, 'en')), #15\n",
        "        (o + 'isMale', isMale(word)),\n",
        "         (o + 'maleAvgAge', maleAvgAge(word)),\n",
        "         (o + 'maleAvgFreq', maleAvgFreq(word)),\n",
        "         (o + 'isFemale', isFemale(word)),\n",
        "         (o + 'femaleAvgAge', femaleAvgAge(word)),\n",
        "         (o + 'femaleAvgFreq', femaleAvgFreq(word))\n",
        "        \n",
        "    ]\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWE_ZtfUTgQJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    \"\"\" The function generates all features\n",
        "    for the word at position i in the\n",
        "    sentence.\"\"\"\n",
        "    features = []\n",
        "    # the window around the token\n",
        "    featlist = [('bias', 1.0)]\n",
        "    features.extend(featlist)\n",
        "\n",
        "    for o in [-1,0,1,2]:\n",
        "        if i+o >= 0 and i+o < len(sent):\n",
        "            word = sent[i+o][0]\n",
        "#             postag = sent[i+o][1]\n",
        "            postag = \"unknown\"\n",
        "            featlist = getfeats(word, postag, o)\n",
        "            features.extend(featlist)\n",
        "        elif i+o<0:\n",
        "            featlist = [('BOS', 1)]\n",
        "            features.extend(featlist)\n",
        "        else:\n",
        "            featlist = [('EOS', 1)]\n",
        "            features.extend(featlist)    \n",
        "    return dict(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HIyr1WKDTidh",
        "colab_type": "code",
        "outputId": "a44914af-b9a6-442d-a619-8d14cf3275b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "    # Load the training data\n",
        "    file = open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/train_word_ner_startidx_dict.pickle','rb')\n",
        "    train_dict = pickle.load(file)\n",
        "    \n",
        "    #ADDED RIGHT NOW!\n",
        "    train_dict = OrderedDict(train_dict)\n",
        "    \n",
        "    file.close()\n",
        "    \n",
        "    train_sents = []\n",
        "    \n",
        "    for k, v in train_dict.items():\n",
        "      train_sents.extend(v)\n",
        "\n",
        "    print(\"train_sents len= \", len(train_sents))\n",
        "\n",
        "    file = open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/dev_word_ner_startidx_dict.pickle','rb')\n",
        "    dev_dict = pickle.load(file)\n",
        "    \n",
        "    #ADDED RIGHT NOW!\n",
        "    dev_dict = OrderedDict(dev_dict)\n",
        "    file.close()\n",
        "    \n",
        "    dev_sents = []\n",
        "    for k, v in dev_dict.items():\n",
        "      dev_sents.extend(v)\n",
        "    \n",
        "    print(\"dev_sents len= \", len(dev_sents))\n",
        "      \n",
        "    file = open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/test_word_ner_startidx_dict.pickle','rb')\n",
        "    test_dict = pickle.load(file)\n",
        "    \n",
        "    #ADDED RIGHT NOW!\n",
        "    test_dict = OrderedDict(test_dict)\n",
        "    file.close()\n",
        "    \n",
        "    test_sents = []\n",
        "    for k, v in test_dict.items():\n",
        "      test_sents.extend(v)\n",
        "    \n",
        "    print(\"test_sents len= \", len(test_sents))\n",
        "    \n",
        "    train_feats = []\n",
        "    train_labels = []\n",
        "\n",
        "    for sent in train_sents:\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent,i)\n",
        "            train_feats.append(feats)\n",
        "            train_labels.append(sent[i][1])\n",
        "\n",
        "    vectorizer = DictVectorizer()\n",
        "    X_train = vectorizer.fit_transform(train_feats)\n",
        "\n",
        "#     # TODO: play with other models\n",
        "#     # model = Perceptron(verbose=1)\n",
        "#     # model = MultinomialNB(alpha=0.01)\n",
        "#     #model = PassiveAggressiveClassifier(C=1.0, fit_intercept=True, early_stopping=False, loss='hinge', average=True, random_state=99)\n",
        "#     # model = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, fit_intercept=True, random_state=99)\n",
        "\n",
        "    print('Training the model')\n",
        "    model = LinearSVC()\n",
        "    \n",
        "    model.fit(X_train, train_labels)\n",
        "    print ('Trained the model')\n",
        "\n",
        "# ##     pickle.dump(model, open('model', 'wb'))\n",
        "\n",
        "    #Training Data\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "\n",
        "#     j = 0\n",
        "#     print(\"Writing to train_results.txt\")\n",
        "#     # format is: word gold pred\n",
        "#     with open(\"/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/train_results.txt\", \"w\") as out:\n",
        "#         for sent in train_sents: \n",
        "#             for i in range(len(sent)):\n",
        "#                 word = sent[i][0]\n",
        "#                 gold = sent[i][1]\n",
        "#                 pred = y_train_pred[j]\n",
        "#                 j += 1\n",
        "#                 out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
        "#         out.write(\"\\n\")\n",
        "\n",
        "    train_pathRead = \"/content/gdrive/Team Drives/cis530/raw_system_data/train/system/\"\n",
        "    train_pathOut = \"/content/gdrive/Team Drives/cis530/raw_system_data/train/system/\"\n",
        "#     #create an .ann format for predictions\n",
        "    createAnnFormat(train_dict, y_train_pred,train_pathRead, train_pathOut)\n",
        "    \n",
        "\n",
        "         \n",
        "    #Dev Data\n",
        "    dev_feats = []\n",
        "    dev_labels = []\n",
        "\n",
        "    # switch to test_sents for your final results\n",
        "    for sent in dev_sents:\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent,i)\n",
        "            dev_feats.append(feats)\n",
        "            dev_labels.append(sent[i][1])\n",
        "\n",
        "    X_dev = vectorizer.transform(dev_feats)\n",
        "    y_dev_pred = model.predict(X_dev)\n",
        "\n",
        "#     j = 0\n",
        "#     print(\"Writing to dev_results.txt\")\n",
        "#     # format is: word gold pred\n",
        "#     with open(\"/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/dev_results.txt\", \"w\") as out:\n",
        "#         for sent in dev_sents: \n",
        "#             for i in range(len(sent)):\n",
        "#                 word = sent[i][0]\n",
        "#                 gold = sent[i][1]\n",
        "#                 pred = y_dev_pred[j]\n",
        "#                 j += 1\n",
        "#                 out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
        "#         out.write(\"\\n\")\n",
        "        \n",
        "    dev_pathRead = \"/content/gdrive/Team Drives/cis530/raw_system_data/dev/system/\"\n",
        "    dev_pathOut = \"/content/gdrive/Team Drives/cis530/raw_system_data/dev/system/\"\n",
        "#     #create an .ann format for predictions\n",
        "    createAnnFormat(dev_dict, y_dev_pred,dev_pathRead, dev_pathOut)\n",
        "\n",
        "    #Test Data\n",
        "    test_feats = []\n",
        "    test_labels = []\n",
        "\n",
        "    # switch to test_sents for your final results\n",
        "    for sent in test_sents:\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent,i)\n",
        "            test_feats.append(feats)\n",
        "            test_labels.append(sent[i][1])\n",
        "\n",
        "    X_test = vectorizer.transform(test_feats)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "#     j = 0\n",
        "#     print(\"Writing to test_results.txt\")\n",
        "#     # format is: word gold pred\n",
        "#     with open(\"/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/test_results.txt\", \"w\") as out:\n",
        "#         for sent in test_sents: \n",
        "#             for i in range(len(sent)):\n",
        "#                 word = sent[i][0]\n",
        "#                 gold = sent[i][1]\n",
        "#                 pred = y_test_pred[j]\n",
        "#                 j += 1\n",
        "#                 out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
        "#         out.write(\"\\n\")\n",
        "        \n",
        "    test_pathRead = \"/content/gdrive/Team Drives/cis530/raw_system_data/test/system/\"\n",
        "    test_pathOut = \"/content/gdrive/Team Drives/cis530/raw_system_data/test/system/\"\n",
        "#     #create an .ann format for predictions\n",
        "    createAnnFormat(test_dict, y_test_pred,test_pathRead, test_pathOut)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_sents len=  8300\n",
            "dev_sents len=  4048\n",
            "test_sents len=  3231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U0Z7S6Ig-OS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create an .ann format for predictions\n",
        "def createAnnFormat(preprocess_dict, y_pred, pathRead, pathOut):   \n",
        "  \n",
        "  \n",
        "    j = 0\n",
        "    for docId, v in preprocess_dict.items():\n",
        "        print('Generating .ann for ',docId)\n",
        "#         #Get the y_pred\n",
        "#         X = preprocess_dict[docId]        \n",
        "        #make predictions and store the results with their tags\n",
        "        txt_start_end = \"\"\n",
        "        tmp_tags=[]\n",
        "        curr_tag = \"\"\n",
        "        \n",
        "        for sent in v:\n",
        "          for item in sent:\n",
        "            word = item[0]\n",
        "            word_start= item[2]\n",
        "            word_end = word_start + len(word) - 1\n",
        "            pred = y_pred[j]   \n",
        "\n",
        "            #if prediction is a beginning, add the tag to the tag list\n",
        "            if pred[0:2] == \"B-\":\n",
        "              if txt_start_end != \"\":\n",
        "                tmp_tags.append((txt_start_end,curr_tag))\n",
        "                txt_start_end= \"\"\n",
        "              curr_tag= pred[2:]\n",
        "              txt_start_end +=\"\"+str(word_start)+\"-\"+str(word_end)+\",\"\n",
        "            #if it is a contuniation keep adding \n",
        "            elif pred[0:2] == \"I-\":\n",
        "              txt_start_end +=\"\"+str(word_start)+\"-\"+str(word_end)+\",\"\n",
        "            j += 1\n",
        "        if txt_start_end != \"\":\n",
        "          tmp_tags.append((txt_start_end,curr_tag))\n",
        "          txt_start_end= \"\"  \n",
        "        #we might be missing some chars in between values, so parse the data and \n",
        "        #get sentence matching that                        \n",
        "        with open(pathRead+docId+\".txt\", \"r\") as f:                                                   \n",
        "          complete_doc = f.read()       \n",
        "\n",
        "        #now for all the text, get their exact match and create tags\n",
        "        tags = []\n",
        "        t_count = 1                        \n",
        "        for i, i_tag in tmp_tags:\n",
        "          splits = i.split(\",\")[:-1]\n",
        "          beginning_split = splits[0].split(\"-\")                 \n",
        "          real_start = int(beginning_split[0])                      \n",
        "          if len(splits) ==1:                  \n",
        "            real_end = int(beginning_split[1])+1\n",
        "          else:\n",
        "            end_split = splits[len(splits)-1].split(\"-\")\n",
        "            real_end = int(end_split[1])+1\n",
        "          tags.append(\"T\"+str(t_count)+\"\\t\"+i_tag+\" \"+str(real_start)+\" \"+str(real_end)+\"\\t\"+ complete_doc[real_start:real_end])\n",
        "          t_count += 1\n",
        "        #now output all these\n",
        "        with open(pathOut+docId+\".ann\", \"w\") as out:\n",
        "          for i in tags:\n",
        "            out.write(i+\"\\n\")                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UyThNPeCr5y-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# Python version of the evaluation script from CoNLL'00-\n",
        "\n",
        "# Intentional differences:\n",
        "# - accept any space as delimiter by default\n",
        "# - optional file argument (default STDIN)\n",
        "# - option to set boundary (-b argument)\n",
        "# - LaTeX output (-l argument) not supported\n",
        "# - raw tags (-r argument) not supported\n",
        "\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from collections import defaultdict, namedtuple\n",
        "\n",
        "ANY_SPACE = '<SPACE>'\n",
        "\n",
        "class FormatError(Exception):\n",
        "    pass\n",
        "\n",
        "Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n",
        "\n",
        "class EvalCounts(object):\n",
        "    def __init__(self):\n",
        "        self.correct_chunk = 0    # number of correctly identified chunks\n",
        "        self.correct_tags = 0     # number of correct chunk tags\n",
        "        self.found_correct = 0    # number of chunks in corpus\n",
        "        self.found_guessed = 0    # number of identified chunks\n",
        "        self.token_counter = 0    # token counter (ignores sentence breaks)\n",
        "\n",
        "        # counts by type\n",
        "        self.t_correct_chunk = defaultdict(int)\n",
        "        self.t_found_correct = defaultdict(int)\n",
        "        self.t_found_guessed = defaultdict(int)\n",
        "\n",
        "def parse_args(argv):\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='evaluate tagging results using CoNLL criteria',\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "    arg = parser.add_argument\n",
        "    arg('-b', '--boundary', metavar='STR', default='-X-',\n",
        "        help='sentence boundary')\n",
        "    arg('-d', '--delimiter', metavar='CHAR', default=ANY_SPACE,\n",
        "        help='character delimiting items in input')\n",
        "    arg('-o', '--otag', metavar='CHAR', default='O',\n",
        "        help='alternative outside tag')\n",
        "    arg('file', nargs='?', default=None)\n",
        "    return parser.parse_args(argv)\n",
        "\n",
        "def parse_tag(t):\n",
        "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
        "    return m.groups() if m else (t, '')\n",
        "\n",
        "def evaluate(iterable, options=None):\n",
        "    if options is None:\n",
        "        options = parse_args([])    # use defaults\n",
        "\n",
        "    counts = EvalCounts()\n",
        "    num_features = None       # number of features per line\n",
        "    in_correct = False        # currently processed chunks is correct until now\n",
        "    last_correct = 'O'        # previous chunk tag in corpus\n",
        "    last_correct_type = ''    # type of previously identified chunk tag\n",
        "    last_guessed = 'O'        # previously identified chunk tag\n",
        "    last_guessed_type = ''    # type of previous chunk tag in corpus\n",
        "\n",
        "    for line in iterable:\n",
        "        line = line.rstrip('\\r\\n')\n",
        "\n",
        "        if options.delimiter == ANY_SPACE:\n",
        "            features = line.split()\n",
        "        else:\n",
        "            features = line.split(options.delimiter)\n",
        "\n",
        "        if num_features is None:\n",
        "            num_features = len(features)\n",
        "        elif num_features != len(features) and len(features) != 0:\n",
        "            raise FormatError('unexpected number of features: %d (%d)' %\n",
        "                              (len(features), num_features))\n",
        "\n",
        "        if len(features) == 0 or features[0] == options.boundary:\n",
        "            features = [options.boundary, 'O', 'O']\n",
        "        if len(features) < 3:\n",
        "            raise FormatError('unexpected number of features in line %s' % line)\n",
        "\n",
        "        guessed, guessed_type = parse_tag(features.pop())\n",
        "        correct, correct_type = parse_tag(features.pop())\n",
        "        first_item = features.pop(0)\n",
        "\n",
        "        if first_item == options.boundary:\n",
        "            guessed = 'O'\n",
        "\n",
        "        end_correct = end_of_chunk(last_correct, correct,\n",
        "                                   last_correct_type, correct_type)\n",
        "        end_guessed = end_of_chunk(last_guessed, guessed,\n",
        "                                   last_guessed_type, guessed_type)\n",
        "        start_correct = start_of_chunk(last_correct, correct,\n",
        "                                       last_correct_type, correct_type)\n",
        "        start_guessed = start_of_chunk(last_guessed, guessed,\n",
        "                                       last_guessed_type, guessed_type)\n",
        "\n",
        "        if in_correct:\n",
        "            if (end_correct and end_guessed and\n",
        "                last_guessed_type == last_correct_type):\n",
        "                in_correct = False\n",
        "                counts.correct_chunk += 1\n",
        "                counts.t_correct_chunk[last_correct_type] += 1\n",
        "            elif (end_correct != end_guessed or guessed_type != correct_type):\n",
        "                in_correct = False\n",
        "\n",
        "        if start_correct and start_guessed and guessed_type == correct_type:\n",
        "            in_correct = True\n",
        "\n",
        "        if start_correct:\n",
        "            counts.found_correct += 1\n",
        "            counts.t_found_correct[correct_type] += 1\n",
        "        if start_guessed:\n",
        "            counts.found_guessed += 1\n",
        "            counts.t_found_guessed[guessed_type] += 1\n",
        "        if first_item != options.boundary:\n",
        "            if correct == guessed and guessed_type == correct_type:\n",
        "                counts.correct_tags += 1\n",
        "            counts.token_counter += 1\n",
        "\n",
        "        last_guessed = guessed\n",
        "        last_correct = correct\n",
        "        last_guessed_type = guessed_type\n",
        "        last_correct_type = correct_type\n",
        "\n",
        "    if in_correct:\n",
        "        counts.correct_chunk += 1\n",
        "        counts.t_correct_chunk[last_correct_type] += 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "def uniq(iterable):\n",
        "  seen = set()\n",
        "  return [i for i in iterable if not (i in seen or seen.add(i))]\n",
        "\n",
        "def calculate_metrics(correct, guessed, total):\n",
        "    tp, fp, fn = correct, guessed-correct, total-correct\n",
        "    p = 0 if tp + fp == 0 else 1.*tp / (tp + fp)\n",
        "    r = 0 if tp + fn == 0 else 1.*tp / (tp + fn)\n",
        "    f = 0 if p + r == 0 else 2 * p * r / (p + r)\n",
        "    return Metrics(tp, fp, fn, p, r, f)\n",
        "\n",
        "def metrics(counts):\n",
        "    c = counts\n",
        "    overall = calculate_metrics(\n",
        "        c.correct_chunk, c.found_guessed, c.found_correct\n",
        "    )\n",
        "    by_type = {}\n",
        "    for t in uniq(list(c.t_found_correct) + list(c.t_found_guessed)):\n",
        "        by_type[t] = calculate_metrics(\n",
        "            c.t_correct_chunk[t], c.t_found_guessed[t], c.t_found_correct[t]\n",
        "        )\n",
        "    return overall, by_type\n",
        "\n",
        "def report(counts, out=None):\n",
        "    if out is None:\n",
        "        out = sys.stdout\n",
        "\n",
        "    overall, by_type = metrics(counts)\n",
        "\n",
        "    c = counts\n",
        "    out.write('processed %d tokens with %d phrases; ' %\n",
        "              (c.token_counter, c.found_correct))\n",
        "    out.write('found: %d phrases; correct: %d.\\n' %\n",
        "              (c.found_guessed, c.correct_chunk))\n",
        "\n",
        "    if c.token_counter > 0:\n",
        "        out.write('accuracy: %6.2f%%; ' %\n",
        "                  (100.*c.correct_tags/c.token_counter))\n",
        "        out.write('precision: %6.2f%%; ' % (100.*overall.prec))\n",
        "        out.write('recall: %6.2f%%; ' % (100.*overall.rec))\n",
        "        out.write('FB1: %6.2f\\n' % (100.*overall.fscore))\n",
        "\n",
        "    for i, m in sorted(by_type.items()):\n",
        "        out.write('%17s: ' % i)\n",
        "        out.write('precision: %6.2f%%; ' % (100.*m.prec))\n",
        "        out.write('recall: %6.2f%%; ' % (100.*m.rec))\n",
        "        out.write('FB1: %6.2f  %d\\n' % (100.*m.fscore, c.t_found_guessed[i]))\n",
        "\n",
        "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
        "    # check if a chunk ended between the previous and current word\n",
        "    # arguments: previous and current chunk tags, previous and current types\n",
        "    chunk_end = False\n",
        "\n",
        "    if prev_tag == 'E': chunk_end = True\n",
        "    if prev_tag == 'S': chunk_end = True\n",
        "\n",
        "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
        "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
        "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
        "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
        "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
        "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
        "\n",
        "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
        "        chunk_end = True\n",
        "\n",
        "    # these chunks are assumed to have length 1\n",
        "    if prev_tag == ']': chunk_end = True\n",
        "    if prev_tag == '[': chunk_end = True\n",
        "\n",
        "    return chunk_end\n",
        "\n",
        "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
        "    # check if a chunk started between the previous and current word\n",
        "    # arguments: previous and current chunk tags, previous and current types\n",
        "    chunk_start = False\n",
        "\n",
        "    if tag == 'B': chunk_start = True\n",
        "    if tag == 'S': chunk_start = True\n",
        "\n",
        "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
        "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
        "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
        "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
        "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
        "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
        "\n",
        "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
        "        chunk_start = True\n",
        "\n",
        "    # these chunks are assumed to have length 1\n",
        "    if tag == '[': chunk_start = True\n",
        "    if tag == ']': chunk_start = True\n",
        "\n",
        "    return chunk_start\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xj36nzZ0sRSD",
        "colab_type": "code",
        "outputId": "e5e7e6ab-8744-4b63-c22b-4d3335da8f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "#Evaluate training results\n",
        "\n",
        "with open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/train_results.txt') as f:\n",
        "    counts = evaluate(f)\n",
        "report(counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 157882 tokens with 8616 phrases; found: 8968 phrases; correct: 8265.\n",
            "accuracy:  99.47%; precision:  92.16%; recall:  95.93%; FB1:  94.01\n",
            "            CALLE: precision:  65.08%; recall:  82.64%; FB1:  72.81  819\n",
            "     CENTRO_SALUD: precision:  60.00%; recall:  75.00%; FB1:  66.67  5\n",
            "CORREO_ELECTRONICO: precision:  88.00%; recall:  98.51%; FB1:  92.96  375\n",
            "EDAD_SUJETO_ASISTENCIA: precision:  91.30%; recall:  96.04%; FB1:  93.61  851\n",
            "FAMILIARES_SUJETO_ASISTENCIA: precision:  84.76%; recall:  80.35%; FB1:  82.49  164\n",
            "           FECHAS: precision:  99.38%; recall:  98.18%; FB1:  98.78  975\n",
            "         HOSPITAL: precision:  70.05%; recall:  83.33%; FB1:  76.12  207\n",
            " ID_ASEGURAMIENTO: precision:  98.62%; recall: 100.00%; FB1:  99.30  289\n",
            "ID_CONTACTO_ASISTENCIAL: precision: 100.00%; recall:  98.08%; FB1:  99.03  51\n",
            "ID_SUJETO_ASISTENCIA: precision:  98.57%; recall:  97.88%; FB1:  98.22  421\n",
            "ID_TITULACION_PERSONAL_SANITARIO: precision:  98.88%; recall:  99.72%; FB1:  99.29  356\n",
            "      INSTITUCION: precision:  71.88%; recall:  71.88%; FB1:  71.88  64\n",
            "NOMBRE_PERSONAL_SANITARIO: precision:  95.32%; recall:  98.85%; FB1:  97.05  812\n",
            "NOMBRE_SUJETO_ASISTENCIA: precision:  97.16%; recall:  97.16%; FB1:  97.16  811\n",
            "       NUMERO_FAX: precision:  62.50%; recall:  71.43%; FB1:  66.67  8\n",
            "  NUMERO_TELEFONO: precision:  82.22%; recall:  90.24%; FB1:  86.05  45\n",
            "OTROS_SUJETO_ASISTENCIA: precision:  62.50%; recall:  71.43%; FB1:  66.67  8\n",
            "             PAIS: precision:  95.88%; recall:  99.07%; FB1:  97.45  558\n",
            "        PROFESION: precision:  78.95%; recall:  75.00%; FB1:  76.92  19\n",
            "SEXO_SUJETO_ASISTENCIA: precision:  99.31%; recall:  99.86%; FB1:  99.58  724\n",
            "       TERRITORIO: precision:  95.45%; recall:  97.11%; FB1:  96.27  1406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Im2FtrjuK4_",
        "colab_type": "code",
        "outputId": "d5f5c308-cca9-4c79-ff1f-9e7cd1a2835e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "#Evaluate dev results\n",
        "\n",
        "with open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/dev_results.txt') as f:\n",
        "    counts = evaluate(f)\n",
        "report(counts)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 74001 tokens with 4320 phrases; found: 4696 phrases; correct: 2943.\n",
            "accuracy:  96.82%; precision:  62.67%; recall:  68.12%; FB1:  65.28\n",
            "            CALLE: precision:  14.72%; recall:  27.44%; FB1:  19.16  591\n",
            "     CENTRO_SALUD: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
            "CORREO_ELECTRONICO: precision:  93.48%; recall:  96.63%; FB1:  95.03  184\n",
            "EDAD_SUJETO_ASISTENCIA: precision:  85.89%; recall:  90.98%; FB1:  88.36  411\n",
            "FAMILIARES_SUJETO_ASISTENCIA: precision:  70.49%; recall:  58.90%; FB1:  64.18  61\n",
            "           FECHAS: precision:  54.90%; recall:  92.92%; FB1:  69.02  909\n",
            "         HOSPITAL: precision:  37.60%; recall:  47.96%; FB1:  42.15  125\n",
            " ID_ASEGURAMIENTO: precision:  57.72%; recall:  64.66%; FB1:  60.99  149\n",
            "ID_CONTACTO_ASISTENCIAL: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
            "ID_EMPLEO_PERSONAL_SANITARIO: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
            "ID_SUJETO_ASISTENCIA: precision:  61.11%; recall:   5.21%; FB1:   9.61  18\n",
            "ID_TITULACION_PERSONAL_SANITARIO: precision:  79.40%; recall:  94.61%; FB1:  86.34  199\n",
            "      INSTITUCION: precision:  13.33%; recall:  10.34%; FB1:  11.65  45\n",
            "NOMBRE_PERSONAL_SANITARIO: precision:  41.99%; recall:  48.82%; FB1:  45.15  443\n",
            "NOMBRE_SUJETO_ASISTENCIA: precision:  54.50%; recall:  59.43%; FB1:  56.86  422\n",
            "       NUMERO_FAX: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
            "  NUMERO_TELEFONO: precision:  41.18%; recall:  31.82%; FB1:  35.90  17\n",
            "OTROS_SUJETO_ASISTENCIA: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
            "             PAIS: precision:  97.33%; recall:  96.23%; FB1:  96.77  262\n",
            "        PROFESION: precision:   0.00%; recall:   0.00%; FB1:   0.00  2\n",
            "SEXO_SUJETO_ASISTENCIA: precision:  98.27%; recall:  99.42%; FB1:  98.84  347\n",
            "       TERRITORIO: precision:  90.77%; recall:  63.46%; FB1:  74.70  509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ODHOwcngr-ro",
        "colab_type": "code",
        "outputId": "c9afd055-ee88-4427-9460-548424379255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Evaluate test results\n",
        "\n",
        "with open('/content/gdrive/Team Drives/cis530/preprocessed_data/word_ner_startindex_dict/test_results.txt') as f:\n",
        "    counts = evaluate(f)\n",
        "report(counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 66194 tokens with 3516 phrases; found: 3887 phrases; correct: 2383.\n",
            "accuracy:  96.88%; precision:  61.31%; recall:  67.78%; FB1:  64.38\n",
            "            CALLE: precision:  15.43%; recall:  29.48%; FB1:  20.26  512\n",
            "     CENTRO_SALUD: precision:  16.67%; recall:  33.33%; FB1:  22.22  6\n",
            "CORREO_ELECTRONICO: precision:  88.08%; recall:  98.52%; FB1:  93.01  151\n",
            "EDAD_SUJETO_ASISTENCIA: precision:  83.38%; recall:  91.08%; FB1:  87.06  355\n",
            "FAMILIARES_SUJETO_ASISTENCIA: precision:  70.83%; recall:  59.30%; FB1:  64.56  72\n",
            "           FECHAS: precision:  52.64%; recall:  98.19%; FB1:  68.54  720\n",
            "         HOSPITAL: precision:  44.04%; recall:  53.33%; FB1:  48.24  109\n",
            " ID_ASEGURAMIENTO: precision:  46.75%; recall:  56.25%; FB1:  51.06  154\n",
            "ID_CONTACTO_ASISTENCIAL: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
            "ID_SUJETO_ASISTENCIA: precision:  50.00%; recall:   0.61%; FB1:   1.21  2\n",
            "ID_TITULACION_PERSONAL_SANITARIO: precision:  73.14%; recall:  93.43%; FB1:  82.05  175\n",
            "      INSTITUCION: precision:   0.00%; recall:   0.00%; FB1:   0.00  7\n",
            "NOMBRE_PERSONAL_SANITARIO: precision:  39.56%; recall:  47.06%; FB1:  42.99  364\n",
            "NOMBRE_SUJETO_ASISTENCIA: precision:  57.28%; recall:  57.64%; FB1:  57.46  316\n",
            "       NUMERO_FAX: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
            "  NUMERO_TELEFONO: precision:   0.00%; recall:   0.00%; FB1:   0.00  4\n",
            "OTROS_SUJETO_ASISTENCIA: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
            "             PAIS: precision:  97.21%; recall:  95.00%; FB1:  96.09  215\n",
            "        PROFESION: precision:  20.00%; recall:  25.00%; FB1:  22.22  5\n",
            "SEXO_SUJETO_ASISTENCIA: precision:  96.59%; recall: 100.00%; FB1:  98.26  293\n",
            "       TERRITORIO: precision:  88.50%; recall:  64.22%; FB1:  74.43  426\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}